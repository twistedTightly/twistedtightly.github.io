<!doctype html>
<html class="no-js" lang="">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <title>Maribeth Rauh</title>
        <meta name="description" content="well aren't you a curious one...">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <link rel="manifest" href="site.webmanifest">
        <link href="https://fonts.googleapis.com/css2?family=Bentham&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Rubik+Glitch&display=swap" rel="stylesheet">
        <link rel="apple-touch-icon" href="icon.png">
        <!-- TODO: Place favicon.ico in the root directory -->

        <link rel="stylesheet" href="css/main.css">
    </head>
    <body>
        <!--[if lte IE 9]>
            <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
        <![endif]-->
        <div id="main-content-container">
          <div class="content-bar" id="name-header">
            <h1>maribeth rauh</h1>
            <p>Research Engineer</p>
            <p>Sociotechnical translation and evaluation</p>
          </div>
          <div class="content-bar" id="hair">
            <img src="img/hair.jpg"/>
            <div id="about-me">
              <p>Few things fascinate me more than the meeting of the digital and the messiness of analog reality. I am motivated by how technology is being used here and now, for better and for worse.</p>
              <p>Since 2021, I have focused on evaluations of generative AI models. Evaluation is a place where what we value is explicitly codified. In machine learning, this usually means translating vague notions of "good" into quantifiable numbers. Delving into the challenges that come with doing so endlessly intrigues me.</p>
              <p>I am also passionate about bridging the gap between engineers / machine learning researchers and social scientists, journalists, and others grappling with the societal impact of AI. This sociotechnical translation, in both directions, is critical for ensuring AI systems do not perpetuate or exacerbate existing inequalities. This will be the default without dedicated interdisciplinary work, and the central motivation of my career is to use translation and evaluation to resist that outcome.</p>
            </div>
            <div class="placeholder"></div>
          </div>

          <h3>Selected Publications</h3>
          <div class="content-bar heather">
            <img src="img/tryfan.jpg"/>
            <div class="publications">
              <p class="pub-section">I have led research on how to improve safety evaluations of generative AI based on my experience evaluating DeepMind's LLMs:</p>
              <p class="publication"><small>Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Ramona Comanescu, Canfer Akbulut, Tom Stepleton, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac, Laura Weidinger.</small> 2024. <b>Gaps in the Safety Evaluation of Generative AI.</b> <a href="https://ojs.aaai.org/index.php/AIES/article/view/31717">AIES 2024</a>.</p>
              <p class="publication"><small>Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang, Johannes Welbl, Laura Weidinger, Sumanth Dathathri, Amelia Glaese, Geoffrey Irving, Iason Gabriel, William Isaac, Lisa Anne Hendricks.</small> 2022. <b>Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models.</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/9ca22870ae0ba55ee50ce3e2d269e5de-Abstract-Datasets_and_Benchmarks.html">NeurIPS 2022 - Datasets and Benchmarks Track</a>.</p>
            </div>
            <div class="placeholder"></div>
          </div>
          
            
          <div class="content-bar heather">
            <img src="img/heather.jpg"/>
            <div class="publications">
              <p class="pub-section">I conducted safety evaluations of LLMs at DeepMind, with a focus on fairness and toxicity:</p>
              <p class="publication llm-papers"><a href="https://arxiv.org/abs/2312.11805">Gemini: A Family of Highly Capable Multimodal Models</a>, 2023</p>
              <p class="publication llm-papers"><a href="https://arxiv.org/abs/2209.14375">Improving alignment of dialogue agents via targeted human judgements</a>, 2022</p>
              <p class="publication llm-papers"><a href="https://arxiv.org/abs/2112.11446">Scaling Language Models: Methods, Analysis & Insights from Training Gopher</a>, 2021</p>
            </div>
            <div class="placeholder"></div>
          </div>

          <div class="content-bar heather">
            <img src="img/eryri.jpg"/>
            <div class="publications">
              <p class="pub-section">I conducted sociotechnical research on the risks of generative AI and how to address that, as early as 2021:</p>
              <p class="publication"><small>Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne Hendricks, Verena Rieser, Hasan Iqbal, Nenad Tomašev, Ira Ktena, Zachary Kenton, Mikel Rodriguez, Seliem El-Sayed, Sasha Brown, Canfer Akbulut, Andrew Trask, Edward Hughes, A. Stevie Bergman, Renee Shelby, Nahema Marchal, Conor Griffin, Juan Mateos-Garcia, Laura Weidinger, Winnie Street, Benjamin Lange, Alex Ingerman, Alison Lentz, Reed Enger, Andrew Barakat, Victoria Krakovna, John Oliver Siy, Zeb Kurth-Nelson, Amanda McCroskery, Vijay Bolina, Harry Law, Murray Shanahan, Lize Alberts, Borja Balle, Sarah de Haas, Yetunde Ibitoye, Allan Dafoe, Beth Goldberg, Sébastien Krier, Alexander Reese, Sims Witherspoon, Will Hawkins, Maribeth Rauh, Don Wallace, Matija Franklin, Josh A. Goldstein, Joel Lehman, Michael Klenk, Shannon Vallor, Courtney Biles, Meredith Ringel Morris, Helen King, Blaise Agüera y Arcas, William Isaac, James Manyika.</small> 2024. <b>The Ethics of Advanced AI Assistants.</b> <a href="https://arxiv.org/abs/2404.16244">Arxiv</a>.</p>
              <p class="publication"><small>Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac.</small> 2023. <b>Sociotechnical Safety Evaluation of Generative AI Systems.</b> <a href="https://arxiv.org/abs/2310.11986">Arxiv</a>.</p>
              <p class="publication"><small>A. Stevie Bergman, Lisa Anne Hendricks, Maribeth Rauh, Boxi Wu, William Agnew, Markus Kunesch, Isabella Duan, Iason Gabriel, and William Isaac.</small> 2023. <b>Representation in AI Evaluations.</b> <a href="https://dl.acm.org/doi/10.1145/3593013.3594019">ACM FAccT 2023</a>.</p>
              <p class="publication"><small>Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel.</small> 2022. <b>Taxonomy of Risks posed by Language Models.</b> <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3533088">ACM FAccT 2022</a>.</p>
            </div>
            <div class="placeholder"></div>
          </div>
          <div class="content-bar" id="techno">
            <img src="img/techno.jpg"/>
            <div class="placeholder"></div>
          </div>
        </div>  <!-- id=main-content-container -->

        <!-- TODO: Add analytics -->
    </body>
</html>
